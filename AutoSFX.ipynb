{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoSFX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import classify\n",
    "import scene_understanding\n",
    "import sync\n",
    "from PIL import Image\n",
    "import openai\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import cv2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(scene_understanding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'classify' from '/home/s5614279/Master Project/AutoSFX/classify.py'>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"/home/s5614279/Master Project/running.mp4\"  # your video path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SceneUnderstanding object\n",
    "scene_understanding = scene_understanding.SceneUnderstanding()\n",
    "\n",
    "\n",
    "resized_frame = scene_understanding.process_video(video_path)\n",
    "#frame_tags = []\n",
    "#frame_context = []\n",
    "\n",
    "frame_values = []\n",
    "frame_objects = []\n",
    "\n",
    "#output_dir = 'output_frames'\n",
    "#os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "#for i, frame in enumerate(resized_frame[::15]):\n",
    "for i, frame in enumerate(resized_frame[::5]):\n",
    "    pil_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    #tags, context = scene_understanding.analyze_image(pil_image)\n",
    "    #frame_tags.append(tags)\n",
    "    #frame_context.append(context)\n",
    "\n",
    "    values, objects = scene_understanding.analyze_image(pil_image)\n",
    "    frame_score = {\n",
    "\n",
    "    }\n",
    "    frame_values.append(values)\n",
    "    frame_objects.append(objects)\n",
    "\n",
    "    #image_save_path = os.path.join(output_dir, f'frame_{i:04d}.jpg')\n",
    "    #pil_image.save(image_save_path)\n",
    "\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "openai.api_key = api_key\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You will be provided with a context describing a scene, and your task is to give the sound suggestions that collectively form the audio landscape of the described scene\"},\n",
    "    {\"role\": \"user\", \"content\": f\"What do I hear in the following video description, give me a list: {frame_context[0]}\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify = classify.Classify()\n",
    "\n",
    "resized_frame = classify.process_video(video_path)\n",
    "\n",
    "frame_values = []\n",
    "frame_objects = []\n",
    "frame_scores = []\n",
    "\n",
    "for i, frame in enumerate(resized_frame[::2]):\n",
    "    pil_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    values, objects = classify.recognize_objects(pil_image)\n",
    "    frame_score = {\n",
    "        'object': objects, #top 5 \n",
    "        'score': values\n",
    "    }\n",
    "    frame_values.append(values)\n",
    "    frame_objects.append(objects)\n",
    "    frame_scores.append(frame_score)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can see the interfer result of every frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object in the scene:  Footsteps walking running  0.4453125\n"
     ]
    }
   ],
   "source": [
    "print(f\"Object in the scene:  {frame_objects[103][0]}  {frame_values[103][0]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Dog': [], 'Rain': [], 'Crying baby': [], 'Door knock': [], 'Helicopter': [], 'Horse': [], 'Rooster': [], 'Sea waves': [], 'Sneezing': [], 'Mouse click': [], 'Chainsaw': [], 'Pig': [], 'Crackling fire': [], 'Clapping': [], 'Keyboard typing': [], 'Siren': [], 'Cow': [], 'Crickets': [], 'Breathing': [], 'Door, wood creaks': [], 'Car horn': [], 'Frog': [], 'Chirping birds': [], 'Coughing': [], 'Can opening': [], 'Engine': [], 'Cat': [], 'Water drops': [], 'Footsteps walking running': [(0, 102, 3.4), (106, 115, 0.3), (117, 122, 0.16666666666666666), (138, 141, 0.1)], 'Washing machine': [], 'Train': [], 'Hen': [], 'Wind': [], 'Laughing': [], 'Vacuum cleaner': [], 'Church bells': [], 'Insects (flying)': [], 'Pouring water': [], 'Brushing teeth': [], 'Clock alarm': [], 'Airplane': [], 'Sheep': [], 'Toilet flush': [], 'Snoring': [], 'Clock tick': [], 'Fireworks': [], 'Crow': [], 'Thunderstorm': [], 'Drinking, sipping': [], 'Glass breaking': [], 'Hand saw': []}\n"
     ]
    }
   ],
   "source": [
    "syncer = sync.ObjectIntervalSync(video_path)\n",
    "syncer.analyze_frames()\n",
    "syncer.calculate_intervals()\n",
    "intervals = syncer.get_intervals()\n",
    "print(intervals)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the intervals, and then we can retrieve the sound effects audio based on the intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import retrieve  # 假设文件名为retrieve.py\n",
    "\n",
    "# 动态传入视频路径\n",
    "video_path = \"/path/to/your/video.mp4\"  # 替换为用户输入的视频路径\n",
    "csv_path = \"/mnt/data/esc50.csv\"  # 替换为实际的CSV路径\n",
    "audio_folder = \"/path/to/audio/folder\"  # 替换为实际的音频文件夹路径\n",
    "\n",
    "# 调用retrieve_audio函数\n",
    "matched_audios = retrieve.retrieve_audio(video_path, csv_path, audio_folder)\n",
    "\n",
    "# 打印匹配结果\n",
    "print(matched_audios)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
